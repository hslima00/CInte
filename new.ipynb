{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs that we'll need for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import percentile\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import operator\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#file created by us that has functions \n",
    "from funcs import * \n",
    "\n",
    "\n",
    "#just to remove the warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the project into steps\n",
    "\n",
    "1. Read the file\n",
    "2. Remove Outliers  \n",
    "   1. Quartiles\n",
    "   2. Deviation **(method used)**\n",
    "   3. By hand in special cases \n",
    "3. Normalization and standardization of the data\n",
    "   1. Z-Score\n",
    "   2. Min Max **(method used)**\n",
    "4. Split the data into train, test and validation sets\n",
    "5. Train the NN \n",
    "   1. KFold \n",
    "   2. Regular train with train set, validation and test set\n",
    "6. Adjust parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file and analyze data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"Proj1_Dataset.csv\", sep=\",\", decimal=\".\")\n",
    "\n",
    "data_df=parse_date_time(data_df)\n",
    "\n",
    "data_df.to_csv(\"Proj1_Dataset_changed.csv\")\n",
    "\n",
    "\n",
    "#run to view the data\n",
    "#plot_data(data_df, temperature=True, CO2=True, PIR=True, light=True)\n",
    "\n",
    "#scatter_plot(data_df, temperature=True, C02_PIR=False, light=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outliers and Interpolate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             S1Temp        S2Temp        S3Temp       S1Light       S2Light  \\\n",
      "count  10128.000000  10127.000000  10129.000000  10129.000000  10129.000000   \n",
      "mean      20.424883     20.553337     20.003294     62.218185     58.250864   \n",
      "std        0.415856      0.663691      0.543616    131.357611    142.220513   \n",
      "min        0.000000     19.660000    -12.320000      0.000000      0.000000   \n",
      "25%       20.130000     20.140000     19.650000      0.000000      0.000000   \n",
      "50%       20.330000     20.340000     19.910000      0.000000      0.000000   \n",
      "75%       20.672500     20.700000     20.310000     28.000000     30.000000   \n",
      "max       21.380000     24.000000     21.180000   5500.000000    516.000000   \n",
      "\n",
      "            S3Light           CO2          PIR1          PIR2       Persons  \n",
      "count  10129.000000  10128.000000  10129.000000  10129.000000  10129.000000  \n",
      "mean      80.774706    474.081754      0.107612      0.094382      0.452068  \n",
      "std      661.501771    204.196690      0.309905      0.292375      0.940729  \n",
      "min        0.000000    345.000000      0.000000      0.000000      0.000000  \n",
      "25%        0.000000    355.000000      0.000000      0.000000      0.000000  \n",
      "50%        0.000000    360.000000      0.000000      0.000000      0.000000  \n",
      "75%      114.000000    531.250000      0.000000      0.000000      0.000000  \n",
      "max    65536.000000   1270.000000      1.000000      1.000000      3.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using ´describe()´ we realized the following:\n",
    "\n",
    "- The dataframe is 10129 rows long and some values return a count of, for example, 10127 rows, therefore we need to fill in the data where this values are missing \n",
    "- If we look closely to the mean and max values of each collumn we realize that there are some outliers due to the discrepancy of the values. Due to the ammount of data we'll simply drop the entire row where the value is found. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed outlined from index  56 from  S1Temp with value of: 0.0\n",
      "Removed outlined from index  1188 from  S3Temp with value of: -12.32\n",
      "Removed outlined from index  3760 from  S1Light with value of: 5500\n",
      "Removed outlined from index  2800 from  S3Light with value of: 65536\n"
     ]
    }
   ],
   "source": [
    "data_df = drop_outliners(data_df, threshold=6,\n",
    "                         collumn_to_remove_outliers=\n",
    "                         [\"S1Temp\", \"S2Temp\",\"S3Temp\",\n",
    "                          \"CO2\",\"PIR1\", \"PIR2\",\"S1Light\",\n",
    "                           \"S2Light\",\"S3Light\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate NaN values\n",
    "data_df = data_df.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "\n",
    "\n",
    "#run to view the data with outliners removed\n",
    "#plot_data(data_df, temperature=True, CO2=True, PIR=True, light=True)\n",
    "\n",
    "#scatter_plot(data_df, temperature=True, C02_PIR=False, light=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data (min max method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test sets\n",
    "train, test = train_test_split(data_df, test_size=0.3, shuffle=True, random_state=1)\n",
    "test, val = train_test_split(test, test_size=0.5, shuffle=True, random_state=3)\n",
    "# split into input and ou\n",
    "y_train_df = train['Persons']\n",
    "x_train_df = train.drop(['Persons'], axis=1)\n",
    "y_test_df = test['Persons']\n",
    "x_test_df = test.drop(['Persons'], axis=1)      \n",
    "y_val_df = val['Persons']\n",
    "x_val_df = val.drop(['Persons'], axis=1)\n",
    "\n",
    "\n",
    "# Normalize data with Min Max \n",
    "x_train_df, x_train_df_min, x_train_df_max = normalize_train_set(x_train_df)\n",
    "x_test_df = normalize_test_set(x_test_df, x_train_df_min, x_train_df_max)\n",
    "x_val_df = normalize_test_set(x_val_df, x_train_df_min, x_train_df_max)\n",
    "#plot_data(x_test_df, temperature=True, CO2=True, light=True)\n",
    "\n",
    "\n",
    "x_train_df = x_train_df.drop(['DateTime'], axis=1)\n",
    "x_val_df = x_val_df.drop(['DateTime'], axis=1)\n",
    "x_test_df = x_test_df.drop(['DateTime'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the NN (K-Fold method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9922425952045134, 0.9851904090267983, 0.9929428369795342, 0.9830628087508821, 0.9865913902611151]\n",
      "precision of each fold - [0.9784492883157706, 0.9490874363327674, 0.9759823623131549, 0.9571879808261332, 0.9537538021307417]\n",
      "recall of each fold - [0.9723669899392731, 0.9561312752758087, 0.9863047148841584, 0.9448788623295393, 0.9663676144383948]\n",
      "Avg accuracy : 0.9880060080445686\n",
      "Avg precision : 0.9628921739837135\n",
      "Avg recall : 0.9652098913734349\n",
      "\n",
      "\n",
      "Accuracy: 0.9855167873601053\n",
      "Precision: 0.9688516652672823\n",
      "Recall: 0.9569444444444445\n",
      "Confusion:\n",
      " [[1185    0    0    0]\n",
      " [   0   59    1    0]\n",
      " [   0    0  136    8]\n",
      " [   6    0    7  117]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train scikit NN\n",
    "from unittest import result\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "#Implementing cross validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=([7,5]), random_state=21, learning_rate_init=0.04)\n",
    "\n",
    "\n",
    "acc_score = []\n",
    "press_score = []\n",
    "rec_score = []\n",
    "\n",
    "x = x_train_df\n",
    "y = y_train_df \n",
    "\n",
    "x_test = x_test_df\n",
    "y_test = y_test_df\n",
    "\n",
    "for train_index , test_index in kf.split(x):\n",
    "    x_train_1, x_val_1 = x.iloc[train_index,:],x.iloc[test_index,:]\n",
    "    y_train_1 , y_val_1 = y.iloc[train_index] , y.iloc[test_index]\n",
    "     \n",
    "    model.fit(x_train_1,y_train_1)\n",
    "    pred_values = model.predict(x_val_1)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_val_1)\n",
    "    press = precision_score(pred_values,  y_val_1, average='macro')\n",
    "    recall = recall_score(pred_values,  y_val_1, average='macro')\n",
    "    \n",
    "    acc_score.append(acc)\n",
    "    press_score.append(press)\n",
    "    rec_score.append(recall)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_press_score = sum(press_score)/k\n",
    "avg_recall_score = sum(rec_score)/k\n",
    "\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('precision of each fold - {}'.format(press_score))\n",
    "print('recall of each fold - {}'.format(rec_score))\n",
    "\n",
    "\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "print('Avg precision : {}'.format(avg_press_score))\n",
    "print('Avg recall : {}'.format(avg_recall_score))\n",
    "\n",
    "y_pred_1 = model.predict(x_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy:\",accuracy_score(y_test_df, y_pred_1))\n",
    "print(\"Precision:\",precision_score(y_test_df, y_pred_1, average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test_df, y_pred_1, average='macro'))\n",
    "print(\"Confusion:\\n\",confusion_matrix(y_test_df, y_pred_1))    \n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556    0\n",
      "2657    0\n",
      "1600    0\n",
      "8326    2\n",
      "4383    0\n",
      "       ..\n",
      "2521    0\n",
      "5576    0\n",
      "3615    3\n",
      "7779    0\n",
      "9132    0\n",
      "Name: Persons, Length: 2127, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519\n",
      "1519\n",
      "Validate\n",
      "Accuracy: 0.9881500987491771\n",
      "Precision: 0.9648080542950181\n",
      "Recall: 0.9673324619520353\n",
      "Confusion:\n",
      " [[1188    2    0    1]\n",
      " [   1   76    0    0]\n",
      " [   0    0  129    3]\n",
      " [   0    0   11  108]]\n",
      "\n",
      "\n",
      "\n",
      "Test\n",
      "Accuracy: 0.9842001316655694\n",
      "Precision: 0.9607748789339445\n",
      "Recall: 0.9525373931623932\n",
      "Confusion:\n",
      " [[1185    0    0    0]\n",
      " [   0   59    1    0]\n",
      " [   0    1  139    4]\n",
      " [   3    0   15  112]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train scikit NN\n",
    "from unittest import result\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# define model\n",
    "#Config: {'activation': 'relu', 'hidden_layer_sizes': (30, 70), 'learning_rate': 'constant', 'learning_rate_init': 0.1, 'max_iter': 30, 'solver': 'sgd'}\n",
    "model = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=([7,5]), random_state=21, learning_rate_init=0.04)\n",
    "#it uses default the relu to activation\n",
    "\n",
    "    \n",
    "#7,5 (bons resultados)\n",
    "\n",
    "\n",
    "# fit model ignoring DateTime column\n",
    "#drop DateTime column\n",
    "\n",
    "model.fit(x_train_df, y_train_df)\n",
    "\n",
    "\n",
    "\n",
    "# make a prediction\n",
    "y_val_pred = model.predict(x_val_df)\n",
    "\n",
    "print(len(y_val_pred))\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test_df)\n",
    "print(len(y_pred))\n",
    "\n",
    "print(\"Validate\")\n",
    "print(\"Accuracy:\",accuracy_score(y_val_df, y_val_pred))\n",
    "print(\"Precision:\",precision_score(y_val_df, y_val_pred, average='macro'))\n",
    "print(\"Recall:\",recall_score(y_val_df, y_val_pred, average='macro'))\n",
    "print(\"Confusion:\\n\",confusion_matrix(y_val_df, y_val_pred))   \n",
    "print(\"\\n\\n\")\n",
    "print(\"Test\")\n",
    "print(\"Accuracy:\",accuracy_score(y_test_df, y_pred))\n",
    "print(\"Precision:\",precision_score(y_test_df, y_pred, average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test_df, y_pred, average='macro'))\n",
    "print(\"Confusion:\\n\",confusion_matrix(y_test_df, y_pred))    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
